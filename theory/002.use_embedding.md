# 002. Why could we use embedding layers everywhere?

According to the official explanation of **DeepEmbed** in RWKV8 \[1], a trainable high-dimensional vector is learned for each token in the vocabulary within the FFN of every model layer. These can be conceptualized as layer-wise embedding modules. These vectors are trainable during training, but during inference, they can be stored in RAM or SSD. Only a minimal subset of parameters needs to be preloaded per token, which drastically reduces VRAM usage.

During inference, the model can prefetch the embedding vector for a given token index at each layer in advance. This vector is then used to perform *channelwise scaling* of the FFN outputs.

These token-specific embedding vectors form a large yet sparse knowledge base that significantly enhances the model’s ability to store and retrieve world knowledge. Although this adds to the total parameter count, these vectors do not consume GPU memory during inference. Furthermore, during training, Tensor Parallelism (TP) can be used to avoid the bandwidth cost of gradient synchronization in Data Parallelism (DP), and they can also be offloaded to RAM or SSD.

In edge inference scenarios, these vectors can be stored in system memory or loaded on-demand via mechanisms like `mmap`. Each token only incurs an additional memory access cost of several dozen kilobytes, making the method highly suitable for deployment on edge devices.

A concurrent approach, **Per-Layer Embeddings (PLE)** from Google’s Gemma 3n \[2], proposes a similar idea.

My own proposal takes this concept further. In replacing linear layers entirely with attention mechanisms, I propose geting the *top-layer query* and all layers' *key* and *value* vectors directly from trainable embedding layers. This idea is inspired by the findings of Wu and Tu \[3], which show that the top-layer KV representations carry the most information, making it reasonable to focus primarily on them. Since the top-layer KV representations are closely related to the original input embeddings, it becomes feasible to directly substitute them with embedding outputs.

However, recognizing that removing linear layers severely limits model expressive capability, I retain the use of separate KVs across layers (as in traditional Transformers), but with their values sourced entirely from different embedding layer outputs.

To further test the viability of using pure embedding layers, I completely removed all linear layers from the FFN, using DeepEmbed-style channelwise scaling on attention outputs instead. Since removing linear layers eliminates inter-head communication, I introduced a *channel mixing mechanism* within the FFN to allow partial information exchange across attention heads.

---

\[1] DeepEmbed: Peng Bo, [https://x.com/BlinkDL\_AI/status/1926941496684519805](https://x.com/BlinkDL_AI/status/1926941496684519805) and [https://www.rwkv.cn/news/read?id=20250527](https://www.rwkv.cn/news/read?id=20250527)

\[2] PLE: Google, [https://developers.googleblog.com/en/introducing-gemma-3n/](https://developers.googleblog.com/en/introducing-gemma-3n/)

\[3] Layer-Condensed KV Cache for Efficient Inference of Large Language Models: Haoyi Wu, Kewei Tu, [https://arxiv.org/abs/2405.10637](https://arxiv.org/abs/2405.10637)
