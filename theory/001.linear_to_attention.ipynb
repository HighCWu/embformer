{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1894a4d8",
   "metadata": {},
   "source": [
    "# 001. Why could we replace linear layers with attention layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4c04eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bf46c0dc270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe4797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a random linear layer result\n",
    "x = torch.randn(4, 8)\n",
    "fc_weight = torch.randn(32, 8)\n",
    "y1 = x @ fc_weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute `v` of the attention layer inputs\n",
    "x_abs = x.abs()\n",
    "x_sum = x_abs.sum(dim=-1, keepdim=True)\n",
    "attn_weights = (x_abs / x_sum)[:,None]\n",
    "v = fc_weight.T[None] * (x_sum[...,None] * x.sign()[...,None]) # (4, 8, 32) (bsz, seq_len, out_dim)\n",
    "_y = (attn_weights @ v)[:,0]\n",
    "\n",
    "assert torch.allclose(_y, y1)\n",
    "\n",
    "\n",
    "# Recovered a random `logits` (result of `q @ k^T`)\n",
    "logits = torch.log(attn_weights)\n",
    "logits = logits - torch.randn(1)\n",
    "\n",
    "## Verify that softmax is approximate to the original logits\n",
    "softmax_result = torch.softmax(logits, dim=-1)\n",
    "# print(\"Recovered logits:\\n\", logits)\n",
    "# print(\"Re-softmax result:\\n\", softmax_result)\n",
    "# print(\"Original p:\\n\", _attn_weights)\n",
    "\n",
    "assert torch.allclose(softmax_result, attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c696a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovered a random `q` and `k` of the attention layer inputs\n",
    "\n",
    "import math\n",
    "\n",
    "d_k = 8\n",
    "\n",
    "s = logits * math.sqrt(d_k)\n",
    "\n",
    "## Use random factorization: s = q @ k^T\n",
    "## Choose random k, then solve for q = s @ inv(k^T)\n",
    "k = torch.randn(4, 8, d_k)\n",
    "k_inv = torch.linalg.inv(k.transpose(-2, -1))\n",
    "q = s @ k_inv\n",
    "logits_rec = q @ k.transpose(-2, -1) / math.sqrt(d_k)\n",
    "\n",
    "assert torch.allclose(logits_rec, logits, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b31eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the output result of attention is consistent with the output result of linear\n",
    "\n",
    "y2 = F.scaled_dot_product_attention(q, k, v)[:,0]\n",
    "\n",
    "assert torch.allclose(y2, y1, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3319d2c",
   "metadata": {},
   "source": [
    "In fact, the process of backtracking `q`, `k`, and `logits` is not strictly necessary.\n",
    "\n",
    "After eliminating the effect of `sign bits` and `softmax`, \n",
    "we can approximately regard the `input` to a linear layer as the attention layer's computed `attn_weights`,\n",
    "and the linear layer's weight matrix `fc_weight(out_features, in_features)` as the attention layer's value matrix `v`,\n",
    "where the `sequence length` equals `in_features`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
